### Вступление
Каждый, кто хоть раз работал с осциллографом или вольтметром замечал, что напряжение или сила тока в цепи никогда не принимают строго определенное значение, они постоянно меняются, "пляшут" вокруг некоторого значения.   Это так называемые **шумы** или флуктуации, которые присутствуют цепях, как и в любых других физических системах. Для описаня шумов нам понадобится математический аппарат теории вероятностей, так как шум представляет собой случайных процесс, поэтому будет разумно для начала вспомнить необходимую математическую теорию.
### Необходимые знания из теории вероятностей
#### Характеристики случайной величины
**Случайная величина** — это переменная, которая в результате случайного эксперимента принимает одно из множества возможных числовых значений. Мы не можем предсказать точное значение, которое она примет, но можем описать вероятность появления тех или иных значений. Делее мы будем говорить о непрерывных случайных величинах. Случайная величина полностью описывается своей плотностью вероятности $f(x)$.

**Математическое ожидание** - характеристика случайной величины , которая определяется формулой:
$$
E[X] = \int\limits_{-\infty}^{\infty}x \cdot f(x)dx
$$
Математическое ожидание имеет смысл среднего взешенного.

**Дисперсия** определяется через матожидание:
$$
D[X] = E[(X - E[X])^2]
$$
Дисперсия является мерой разброса случайной величины. Можно переписать в другом виде:
$$
D[X] = E[X^2] -E^2[X] 
$$
Иногда это бывает полезно.
Дисперсия имеет размерность квадрата величины, поэтому иногда бывает удобнее пользоваться **среднеквадратичным отклонением**:
$$
\sigma_X = \sqrt{D[X]}
$$
Случайные величины $X$ и $Y$ называются **независимыми**, если для их математических ожиданий выполняется следующее тождество:
$$
E[XY] = E[X]E[Y]
$$
Однако более интуитивное определение получится, если переписать это выражение:
$$
E[X] = E[X|Y] = \frac{E[XY]}{E[Y]} \qquad E[Y]= E[Y|X] = \frac{E[XY]}{E[X]}
$$
Получаем определение условной вероятности. Отсюда видно, что случайные величины независимы, если инфорация о значении одной из них не дает дополнительной информации о величине другой. 

**Совместное распределение двух случайных величин.** Предположим, что у нас есть случайная величина $\zeta$, которая является функцией от двух других случайных величин $\xi$, $\eta$.
$$
\zeta = \phi(\xi, \eta)
$$
Каждая случайная величина обладает своим распределением, поэтому для того, чтобы посчитать, например, матожидание $\zeta$ нужно рассматривать совместное распрделение $\rho(x, y)$:
$$
E[\zeta] = \iint \phi(x, y)\rho(x, y)dxdy
$$

**Ковариация** случайной величины определяется следующей формулой:
$$
cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]
$$
Ковариация показывает, насколько синхронно изменяются две величины, если ковариация равна нулю, величины называют некоррелированными.(не путать с независимостью!). **Коэффициент корреляции** представляет из себя нормированную ковариацию:
$$
\rho_{XY} = \frac{cov(X,Y)}{\sigma_X\sigma_Y} \in [-1, 1]
$$

#### Характеристики случайного процесса. 
Теперь поговорим об основном объекте нашего исследования.
**Случайный процесс** $X(t, \omega)$ - это функция двух переменных:
1) t (Обычно время) - аргумент из некоторого множетва $T$.
2)  Элементарного исхода $\omega$, принадлежащего пространству элементарных исходов $\Omega$. 

Представьте себе завод по производству генераторов шума. Возьмем все поизведенные на заводе генераторы и  одновременно включим их. Аргумент $\omega$ отвечает за выбор генератора шума, а $t$ за выбор момента времени, в который мы смотрим на значения генератора. То есть конкретный генератор символизирует реализацию случайного процесса, которая является функцией времени. Если мы фиксируем время и смотрим значения на разных генераторах, то получаем случайную величину. 

![[generators.png#center|400]]
Здесь стоит сделать важное замечание. Далее мы будем рассматривать только **эргодические** случайные процессы. Все реальные шумы относятся к этому классу. Процесс называется эргодическим, если усреднение по времени одной единственной реализации дает тот же результат, что и усреднение по всему ансамблю в один момент времени. То есть если мы возьмем один единственный генератор шума с нашего завода и вычислим среднее, то получим тот же результат, как если бы усредняли по значениям всех генераторов в фиксированный момент времени.

Эргодический процесс всегда является также и стационарным. Это значит, что его вероятностные характеристики не зависят от времени. Разница между стационарностью и эргодичностью хоть и не велика, но присутствует. Не каждый стационарный процесс является эргодическим. 

Если мы хотим вычислить матожидание или дисперсию для стационарного процесса, мы можем просто зафиксировать произвольный момент  вычислить их через формулу для случайной величины. Плотность вероятности в силу стационарности не зависит от времени.
$$
E[x] = \int x \rho(x)dx
$$

Если же процесс является эргодическим, то вычисление вероятностных характеристик сводится к "усреднению по времени":
$$
E[x] = \lim\limits_{T\rightarrow\infty} \frac{1}{2T}\int\limits_{-T}^{T}x(t)dt \qquad D[x] = \lim\limits_{T\rightarrow\infty} \frac{1}{2T}\int\limits_{-T}^{T}[x(t)-E[x]]^2dt
$$

Для случайных процессов также вводится корреляция. 
Если присутствует два случайных процесса, то рассмаривается **взаимная корреляционная функция**:
$$
\langle x, y \rangle (\tau) = E\left[ x(u)y(\tau-u) \right]
$$
Для одного сигнала рассматривается **автокорреляция**:
$$
\langle x, x \rangle (\tau) = E\left[ x(u)x(\tau-u) \right]
$$
Пользуясь эргодичностью можно аналогично матожиданию и дисперсии заменить усреднение по выборкам на усреднение по времени отдельной реализации:
$$
\langle x, y \rangle (\tau) = \lim\limits_{T \rightarrow\infty}\int\limits_{-T/2}^{T/2}x(u)y(u-\tau)du
$$
(И аналогично для автокорреляции)
Заметим схожесть формулы автокорреляции с формулой свертки, (отличие лишь в порядке $u-t$, поэтому для корреляции коммутативность в отличие от свертки не выполняется)

Теперь мы готовы к изучению теории шумов.

### Шум
Как уже было сказано выше шум является эргодическим, а значит стационарным случайным процессом. Тогда можно вычислять вероятностные характеристики аналогично вычислению характеристик случайных величин. Мощностью шума называется следующее выражение:
$$
\sigma^2 = E[n^2] = \int n^2 p(n)dn
$$
 где $p(n)$ - плотность распределения. Такое обозначение шума введено для того, чтобы сам символ $\sigma$ обозначала **эффективное значение шума**.
Пользуясь эргодичностью сведем вычисление мощности к интегрированию по времени.
$$
\sigma_T^2 = \frac{1}{T}\int\limits^{T/2}_{-T/2}n^2(u)du \approx \frac{1}{N}\sum\limits_{i=1}^N n^2(u_i)
$$
Таким образом можно вычислить приблизительное значение мощности экспериментально. Автокорреляцию шума называют просто **корреляцией шума** и обозначают таким символом:
$$
n^2(t) = \langle n, n \rangle (t) = E[n(u)n(t-u)]
$$
Заметим, что при $t = 0$ корреляция шума равна его мощности.

Для нескольких шумов вводится понятие взаимной корреляции, коротое полностью совпадает с взаимной корреляцией случайных процессов:

$$
\langle n_1, n_2 \rangle (t) = \lim\limits_{T \rightarrow\infty}\int\limits_{-T/2}^{T/2}n_2(u)n_1(u-t)du
$$
Далее мы будем считать, что все шумы, порожденные разными физическими механизмами некоррелированны. (Умная фраза из Григорьева. ну а с чего бы им быто коррелированными)
### Спектральная плотность шума
Это каноническая форма пр-ия Фурье. Пусть будет тут. 
$$
\mathcal{F}\left[f\right](\omega) = v.p.\frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\infty}f(t)e^{-ixt} dt
$$
Спектральная плотность шума $n^2(f)$ определяется следующим образом.
$$
\frac{n^2(f)}{2} =\int\limits_{-\infty}^{\infty}n^2(t)e^{-j2\pi ft} dt = 2 
$$

### Заметки по лекции
Нужно ли говорить пр то, что такое случайный процесс и про разницу между эргодическим и стационарным случайными процессами?
Нужно ли пояснить про плотность вероятности, и привести пример?
Не слишком ли много вводной теорверовской дрисни?

Я так подумал. Можно рассказывать про шумы в общем смысле, даже не обращаясь до поры до времени к цепям. То есть рассказывать просто как про случайные процессы. Что думаешь?
