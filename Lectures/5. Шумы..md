---
noteID: 64175431-97e7-4d16-90a3-34d228113def
---
### Вступление
Каждый, кто хоть раз работал с осциллографом или вольтметром замечал, что напряжение или сила тока в цепи никогда не принимают строго определенное значение, они постоянно меняются, "пляшут" вокруг некоторой величины.   Это так называемые **шумы** или флуктуации, которые присутствуют в цепях, как и в любых других физических системах. Для описаня шумов нам понадобится математический аппарат теории вероятностей, так как шум представляет собой случайный процесс, поэтому будет разумно для начала вспомнить необходимую математическую теорию.
### Необходимые знания из теории вероятностей
#### Характеристики случайной величины
**Случайная величина** — это переменная, которая в результате случайного эксперимента принимает одно из множества возможных числовых значений. Мы не можем предсказать точное значение, которое она примет, но можем описать вероятность появления тех или иных значений. Делее мы будем говорить о непрерывных случайных величинах. Случайная величина полностью описывается своей плотностью вероятности $f(x)$.

**Математическое ожидание** - характеристика случайной величины , которая определяется формулой:
$$
E[X] = \int\limits_{-\infty}^{\infty}x  f(x)dx
$$
Математическое ожидание имеет смысл среднего взешенного.

**Дисперсия** определяется через математическое ожидание:
$$
D[X] = E[(X - E[X])^2]
$$
Дисперсия является мерой разброса случайной величины. Можно переписать в другом виде:
$$
D[X] = E[X^2] -E^2[X] 
$$
Иногда это бывает полезно.
Дисперсия имеет размерность квадрата величины, поэтому иногда бывает удобнее пользоваться **среднеквадратичным отклонением**:
$$
\sigma_X = \sqrt{D[X]}
$$
Случайные величины $X$ и $Y$ называются **независимыми**, если для их математических ожиданий выполняется следующее тождество:
$$
E[XY] = E[X]E[Y]
$$
Однако более интуитивное определение получится, если переписать это выражение:
$$
E[X] = E[X|Y] = \frac{E[XY]}{E[Y]}; \qquad E[Y]= E[Y|X] = \frac{E[XY]}{E[X]}
$$
Получаем определение условной вероятности. Отсюда видно, что случайные величины независимы, если инфорация о значении одной из них не дает дополнительной информации о величине другой. 

**Совместное распределение двух случайных величин.** Предположим, что у нас есть случайная величина $\zeta$, которая является функцией от двух других случайных величин $\xi$, $\eta$.
$$
\zeta = \varphi(\xi, \eta)
$$
Каждая случайная величина обладает своим распределением, поэтому для того, чтобы посчитать, например, матожидание $\zeta$ нужно рассматривать совместное распрделение $\rho(x, y)$:
$$
E[\zeta] = \iint \varphi(x, y)\rho(x, y)dxdy
$$

**Ковариация** случайной величины определяется следующей формулой:
$$
\text{cov}(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]
$$
Ковариация показывает, насколько синхронно изменяются две величины, если ковариация равна нулю, величины называют некоррелированными.(не путать с независимостью!). **Коэффициент корреляции** представляет из себя нормированную ковариацию:
$$
\rho_{XY} = \frac{\text{cov}(X,Y)}{\sigma_X\sigma_Y} \in [-1, 1]
$$

#### Характеристики случайного процесса. 
Теперь поговорим об основном объекте нашего исследования.
**Случайный процесс** $X(t, \omega)$ - это функция двух переменных:
1) t (Обычно время) - аргумент из некоторого множетва $T$.
2)  Элементарного исхода $\omega$, принадлежащего пространству элементарных исходов $\Omega$. 

Представьте себе завод по производству генераторов шума. Возьмем все поизведенные на заводе генераторы и  одновременно включим их. Аргумент $\omega$ отвечает за выбор генератора шума, а $t$ за выбор момента времени, в который мы смотрим на значения генератора. То есть конкретный генератор символизирует реализацию случайного процесса, которая является функцией времени. Если мы фиксируем время и смотрим значения на разных генераторах, то получаем случайную величину. 

![[generators.png#center|400]]
Здесь стоит сделать важное замечание. Далее мы будем рассматривать только **эргодические** случайные процессы. Все реальные шумы относятся к этому классу. Процесс называется эргодическим, если усреднение по времени одной единственной реализации дает тот же результат, что и усреднение по всему ансамблю в один момент времени. То есть если мы возьмем один единственный генератор шума с нашего завода и вычислим среднее, то получим тот же результат, как если бы усредняли по значениям всех генераторов в фиксированный момент времени.

Эргодический процесс всегда является также и стационарным. Это значит, что его вероятностные характеристики не зависят от времени. Разница между стационарностью и эргодичностью хоть и не велика, но присутствует. Не каждый стационарный процесс является эргодическим. 

Если мы хотим вычислить математическое ожидание или дисперсию для стационарного процесса, мы можем просто зафиксировать произвольный момент  вычислить их через формулу для случайной величины. Плотность вероятности в силу стационарности не зависит от времени.
$$
E[x] = \int x \rho(x)dx
$$

Если же процесс является эргодическим, то вычисление вероятностных характеристик сводится к "усреднению по времени":
$$
E[x] = \lim\limits_{T\rightarrow\infty} \frac{1}{2T}\int\limits_{-T}^{T}x(t)dt \qquad D[x] = \lim\limits_{T\rightarrow\infty} \frac{1}{2T}\int\limits_{-T}^{T}[x(t)-E[x]]^2dt
$$

Для случайных процессов также вводится корреляция. 
Если присутствует два случайных процесса, то рассмаривается **взаимная корреляционная функция**:
$$
\langle x, y \rangle (\tau) = E\left[ x(u)y(\tau-u) \right]
$$
Для одного сигнала рассматривается **автокорреляция**:
$$
\langle x, x \rangle (\tau) = E\left[ x(u)x(\tau-u) \right]
$$
Пользуясь эргодичностью можно аналогично матожиданию и дисперсии заменить усреднение по выборкам на усреднение по времени отдельной реализации:
$$
\langle x, y \rangle (\tau) = \lim\limits_{T \rightarrow\infty}\int\limits_{-T/2}^{T/2}x(u)y(u-\tau)du
$$
(И аналогично для автокорреляции)
Заметим схожесть формулы автокорреляции с формулой свертки, (отличие лишь в порядке $u-t$, поэтому для корреляции коммутативность в отличие от свертки не выполняется).

Теперь мы готовы к изучению теории шумов.

### Шум
Как уже было сказано выше шум является эргодическим, а значит стационарным случайным процессом. Тогда можно вычислять вероятностные характеристики аналогично вычислению характеристик случайных величин. Мощностью шума называется следующее выражение:
$$
\sigma^2 = E[n^2] = \int n^2 p(n)dn
$$
 где $p(n)$ - плотность распределения. Такое обозначение шума введено для того, чтобы сам символ $\sigma$ обозначала **эффективное значение шума**.
Пользуясь эргодичностью сведем вычисление мощности к интегрированию по времени.
$$
\sigma_T^2 = \frac{1}{T}\int\limits^{T/2}_{-T/2}n^2(u)du \approx \frac{1}{N}\sum\limits_{i=1}^N n^2(u_i)
$$
Таким образом можно вычислить приблизительное значение мощности экспериментально. Автокорреляцию шума называют просто **корреляцией шума** и обозначают таким символом:
$$
n^2(t) = \langle n, n \rangle (t) = E[n(u)n(t-u)]
$$
Заметим, что при $t = 0$ корреляция шума равна его мощности.

Для нескольких шумов вводится понятие взаимной корреляции, коротое полностью совпадает с взаимной корреляцией случайных процессов:

$$
\langle n_1, n_2 \rangle (t) = \lim\limits_{T \rightarrow\infty}\int\limits_{-T/2}^{T/2}n_2(u)n_1(u-t)du
$$
Далее мы будем считать, что все шумы, порожденные разными физическими механизмами некоррелированны. (Умная фраза из Григорьева. ну а с чего бы им быто коррелированными)
### Спектральная плотность шума
Это каноническая форма прямого и обратного преобразований Фурье. Пусть будет тут. 
$$
\mathcal{F}\left[f\right](\omega) = \text{v.p.}\frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\infty}f(t)e^{-i\omega t} dt; \qquad 
\mathcal{F}^{-1}\left[F\right](t) = \text{v.p.}\frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{\infty}f(t)e^{+i\omega t} d\omega
$$
Спектральная плотность шума $n^2(f)$ определяется следующим образом.
$$
\frac{n^2(f)}{2} =\int\limits_{-\infty}^{\infty}n^2(t)e^{-j2\pi ft} dt 
$$
По сути спекртальная плотность шума это **преобразование Фурье** от корреляции шума. Однако в форма записи несколько отличается, поэтому для ясности  последовательно перейдем от канонической формы к определению $n^2(t)$.
Обычно в физике в отличие от математики преобразование Фурье определяют несколько иначе, перенося коэффициент $\frac{1}{\sqrt{2 \pi}}$ из прямого преобразования в обратное. А также опускают $\text{v.p.}$    
$$
\mathcal{F}\left[f\right](\omega) = \int\limits_{-\infty}^{\infty}f(t)e^{-i\omega t} dt; \qquad 
\mathcal{F}^{-1}\left[F\right](t) = \frac{1}{2\pi}\int\limits_{-\infty}^{\infty}f(t)e^{+i2\pi t} dt
$$
Теперь перейдем от угловой частоты к обычной частоте, измеряемой в Герцах: $\omega = 2 \pi \nu$. При этом изменится и дифференциал: $d\omega = 2\pi d\nu$. Виим, что $2\pi$ сократятся.

$$
\mathcal{F}\left[f\right](\omega) = \int\limits_{-\infty}^{\infty}f(t)e^{-i2\pi t} dt; \qquad 
\mathcal{F}^{-1}\left[F\right](t) =\int\limits_{-\infty}^{\infty}f(t)e^{+i2\pi t} d\omega
$$

Осталось обсудить двойку в знаменателе.
Если мы посмотрим на определение корреляции шума $n^2(t)$ то увидим, что это четная действительная фукнкция. Как известно в таком случае фурье образ также будет вещественной четной функцией. 

$$
\int\limits_{-\infty}^{\infty}n^2(t)e^{-j2\pi ft} dt  = \int\limits_{-\infty}^{\infty}n^2(t)(\cos(-j2\pi ft) + j\sin(-j2\pi ft))dt
$$
Если спектр является четной функцией, значит ее отрицательная ось частот не несет в себе информации. Таким образом ее можно отбросить. Однако для сохранения мощности стоит удвоить положительную часть. В этом и кроется смысл удвоения: $n^2(f)$ - это **односторонняя** плотность, которая во всех точках вдвое больше исходной плотности.

Корреляцию, как обратное преобразование Фурье можно выразить через спектральную плотность шума:

$$
n^2(t) = \int\limits_{-\infty}^{+\infty} \frac{n^2(f)}{2}e^{+j2\pi ft}d f
$$
##### Размерности
Какую размерность имеет $n^2(t)?$  Если сигнал, который мы наблюдаем измеряется в вольтах, то из определения можно увидеть, что корреляция шума имеет размерность $[V^2]$. Двойка в символе корреляции как раз указывает на квадратичность. Заметим, таким образом, что эта величина пропорциональна мощности сигнала, которая имеет размерность $[\frac{V^2}{R}]$. Спектральная плотность определяется через интеграл корреляции по времени, значит ее размерность $[V^2 T]$, а значит спектральная плотность пропорциональна энергии. 

##### Частотная и временная области.
Возможно у вас возник вопрос, зачем вообще нам нужны такие понятия, как корреляция и спекртальная плотность. На самом деле мы уже почти подобрались к ответу на этот вопрос, но для этого, нам нужно вспомнить одну важную концепцию из прошлых лекций. 
Когда мы обсуждали пассивные электрические схемы, мы говорили, что ограничиваемся рассмотрением **линейных** **стационарных** систем. 
Напомню, что система является линейной, если для нее выполняется принцип суперпозиции. То есть если 
Для того, чтобы двигаться дальше нужно освежить в памяти знания по фильтрации. 


Здесь все про фильтрацию 



#### Зачем нужна корреляция и спектральная плотность.








#### Путаница определений
Вместо спектральной плотности иногда используют корень из спектральной плотности Он уже имеет размерность $[V\sqrt{T}]$, поэтому называется эффекивным значением спектральной плотности шума. Название довольно длинное, поэтому его сокращают до "шумового напряжения ", и это порождает путаницу. Нужно уметь различать эти понятия.


